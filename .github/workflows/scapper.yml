# .github/workflows/scraper.yml
name: Scraper

# on:
#   schedule:
#     - cron: '0,30 * * * *' # every 30 minutes at :00 and :30
#   workflow_dispatch:

concurrency:
  group: database-operations
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run scraper
        env:
          MYSQL_DB: ${{ secrets.MYSQL_DB }}
          MYSQL_USER: ${{ secrets.MYSQL_USER }}
          MYSQL_PASSWORD: ${{ secrets.MYSQL_PASSWORD }}
          MYSQL_HOST: ${{ secrets.MYSQL_HOST }}
          MYSQL_PORT: ${{ secrets.MYSQL_PORT }}
        run: |
          python scapper/scraper.py

      - name: Check for changes
        id: check_changes
        run: |
          if [ -n "$(git status --porcelain)" ]; then
            echo "changes=true" >> $GITHUB_OUTPUT
          else
            echo "changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Commit and push changes
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .
          git commit -m "Auto-update: Scraped data $(date '+%Y-%m-%d %H:%M:%S')"
          git pull origin main
          git push

      # Database is now stored in MySQL server, no need to upload artifacts
